# Text Embeddings & NLP Representations

This repository provides examples of **word embeddings** and **text representation techniques** used in Natural Language Processing (NLP).  
It covers both **neural embeddings** (Word2Vec, FastText, GloVe, CoVe) and **traditional statistical methods** (TF-IDF, LDA, SCM).

---

## Topics
### ðŸ”¹ Word Embeddings
- **Word2Vec** â€“ vector representations of words using CBOW and Skip-gram architectures.  
- **FastText** â€“ an extension of Word2Vec that incorporates subword information for better handling of rare words.  
- **GloVe** â€“ Global Vectors for Word Representation, trained on word co-occurrence statistics.  
- **CoVe** â€“ Contextualized Word Vectors from a pre-trained machine translation model.  

### Statistical & Classical NLP
- **TF-IDF** â€“ Term Frequency â€“ Inverse Document Frequency, a simple yet powerful text weighting scheme.  
- **LDA** â€“ Latent Dirichlet Allocation, a probabilistic topic modeling approach.  
- **SCM** â€“ Semantic Content Model, representing semantic similarity across documents.  
